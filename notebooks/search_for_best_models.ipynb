{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b701f00f",
   "metadata": {},
   "source": [
    "# Exploration phase — initial testing\n",
    "Here we try out different models on our data. The comments below explain exactly what is happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a2d83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kedro.framework.session import KedroSession\n",
    "from kedro.framework.startup import bootstrap_project\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.base import clone\n",
    "import time\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    ExtraTreesRegressor,\n",
    "    AdaBoostRegressor,\n",
    "    GradientBoostingRegressor,\n",
    ")\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457b3910581c944e",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = Path.cwd().parent\n",
    "bootstrap_project(project_path)\n",
    "print(f\"Ścieżka projektu: {project_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddcb550",
   "metadata": {},
   "outputs": [],
   "source": [
    "with KedroSession.create(project_path=project_path) as session:\n",
    "    context = session.load_context()\n",
    "    dfs = context.catalog.load(\"circuit_lap_data\")\n",
    "\n",
    "print(f\"Załadowano dane dla {len(dfs)} obwodów:\")\n",
    "for circuit, df in dfs.items():\n",
    "    print(f\"  {circuit}: {df.shape[0]} okrążeń, {df.shape[1]} cech\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a8f986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare regressor configurations for testing\n",
    "\n",
    "# We test many algorithms with parameter tuning using GridSearchCV.\n",
    "# The GridSearchCVs here will be used as templates. For each circuit,\n",
    "# every of the GridSearchCVs below will be cloned and fitted to their data.\n",
    "\n",
    "# GridSearchCV configurations\n",
    "model_searches = {\n",
    "    # Linear regression\n",
    "    \"LinearRegression\": GridSearchCV(\n",
    "        make_pipeline(StandardScaler(), PCA(), LinearRegression()),\n",
    "        {\"pca__n_components\": [0.98, 0.95, 0.9]},\n",
    "    ),\n",
    "    \"RidgeCV\": GridSearchCV(\n",
    "        make_pipeline(StandardScaler(), PCA(), RidgeCV(alphas=(0.1, 1.0, 10.0))),\n",
    "        {\"pca__n_components\": [0.98, 0.95, 0.9]},\n",
    "    ),\n",
    "    \"LassoCV\": GridSearchCV(\n",
    "        make_pipeline(\n",
    "            StandardScaler(),\n",
    "            PCA(),\n",
    "            LassoCV(max_iter=100_000, alphas=[0.001, 0.01, 0.1, 1.0]),\n",
    "        ),\n",
    "        {\"pca__n_components\": [0.98, 0.95, 0.9]},\n",
    "    ),\n",
    "    \"ElasticNetCV\": GridSearchCV(\n",
    "        make_pipeline(\n",
    "            StandardScaler(),\n",
    "            PCA(),\n",
    "            ElasticNetCV(max_iter=100_000, l1_ratio=[0.2, 0.5, 0.8]),\n",
    "        ),\n",
    "        {\"pca__n_components\": [0.98, 0.95, 0.9]},\n",
    "    ),\n",
    "    # Polynomial regression\n",
    "    \"PolynomialLinearRegression\": GridSearchCV(\n",
    "        make_pipeline(\n",
    "            StandardScaler(), PCA(), PolynomialFeatures(), LinearRegression()\n",
    "        ),\n",
    "        {\"polynomialfeatures__degree\": [2, 3], \"pca__n_components\": [0.98, 0.95, 0.9]},\n",
    "    ),\n",
    "    \"PolynomialRidgeCV\": GridSearchCV(\n",
    "        make_pipeline(\n",
    "            StandardScaler(),\n",
    "            PCA(),\n",
    "            PolynomialFeatures(),\n",
    "            RidgeCV(alphas=(0.1, 1.0, 10.0)),\n",
    "        ),\n",
    "        {\"polynomialfeatures__degree\": [2, 3], \"pca__n_components\": [0.98, 0.95, 0.9]},\n",
    "    ),\n",
    "    \"PolynomialLassoCV\": GridSearchCV(\n",
    "        make_pipeline(\n",
    "            StandardScaler(),\n",
    "            PCA(),\n",
    "            PolynomialFeatures(),\n",
    "            LassoCV(max_iter=100_000, alphas=[0.001, 0.01, 0.1]),\n",
    "        ),\n",
    "        {\"polynomialfeatures__degree\": [2, 3], \"pca__n_components\": [0.98, 0.95, 0.9]},\n",
    "    ),\n",
    "    \"PolynomialElasticNetCV\": GridSearchCV(\n",
    "        make_pipeline(\n",
    "            StandardScaler(),\n",
    "            PCA(),\n",
    "            PolynomialFeatures(),\n",
    "            ElasticNetCV(max_iter=100_000, l1_ratio=[0.2, 0.5, 0.8]),\n",
    "        ),\n",
    "        {\"polynomialfeatures__degree\": [2, 3], \"pca__n_components\": [0.98, 0.95, 0.9]},\n",
    "    ),\n",
    "    # Bagging models\n",
    "    \"RandomForestRegressor\": GridSearchCV(\n",
    "        RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "        {\n",
    "            \"n_estimators\": [100, 200, 400],\n",
    "            \"max_depth\": [5, 10, 20, None],\n",
    "            \"min_samples_split\": [2, 5, 10],\n",
    "        },\n",
    "    ),\n",
    "    \"ExtraTreesRegressor\": GridSearchCV(\n",
    "        ExtraTreesRegressor(random_state=42, n_jobs=-1),\n",
    "        {\n",
    "            \"n_estimators\": [100, 200, 400],\n",
    "            \"max_depth\": [5, 10, 20, None],\n",
    "            \"min_samples_split\": [2, 5, 10],\n",
    "        },\n",
    "    ),\n",
    "    # Boosting models\n",
    "    \"AdaBoostRegressor\": GridSearchCV(\n",
    "        AdaBoostRegressor(random_state=42),\n",
    "        {\"n_estimators\": [50, 100, 200], \"learning_rate\": [0.01, 0.1, 0.5, 1.0]},\n",
    "    ),\n",
    "    \"GradientBoostingRegressor\": GridSearchCV(\n",
    "        GradientBoostingRegressor(random_state=42),\n",
    "        {\n",
    "            \"n_estimators\": [100, 200],\n",
    "            \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "            \"max_depth\": [3, 5],\n",
    "            \"subsample\": [0.8, 1.0],\n",
    "        },\n",
    "    ),\n",
    "    \"XGBRegressor\": GridSearchCV(\n",
    "        XGBRegressor(\n",
    "            random_state=42, n_jobs=-1, objective=\"reg:squarederror\", verbosity=0\n",
    "        ),\n",
    "        {\n",
    "            \"n_estimators\": [100, 200, 400],\n",
    "            \"max_depth\": [3, 6, 10],\n",
    "            \"learning_rate\": [0.01, 0.1, 0.3],\n",
    "            \"subsample\": [0.8, 1.0],\n",
    "            \"colsample_bytree\": [0.8, 1.0],\n",
    "        },\n",
    "    ),\n",
    "    # Support vector models\n",
    "    \"SVR_linear\": GridSearchCV(\n",
    "        make_pipeline(StandardScaler(), PCA(), SVR(kernel=\"linear\")),\n",
    "        {\"svr__C\": [0.1, 1, 10, 100], \"pca__n_components\": [0.98, 0.95, 0.9]},\n",
    "    ),\n",
    "    \"SVR_rbf\": GridSearchCV(\n",
    "        make_pipeline(StandardScaler(), PCA(), SVR(kernel=\"rbf\")),\n",
    "        {\n",
    "            \"svr__C\": [0.1, 1, 10],\n",
    "            \"svr__gamma\": [\"scale\", 0.01, 0.1, 1.0],\n",
    "            \"pca__n_components\": [0.98, 0.95, 0.9],\n",
    "        },\n",
    "    ),\n",
    "    # MLP\n",
    "    \"MLPRegressor\": GridSearchCV(\n",
    "        make_pipeline(\n",
    "            StandardScaler(), PCA(), MLPRegressor(max_iter=100_000, random_state=42)\n",
    "        ),\n",
    "        {\n",
    "            \"mlpregressor__hidden_layer_sizes\": [\n",
    "                (16,),\n",
    "                (24,),\n",
    "                (24, 12),\n",
    "                (16, 16),\n",
    "                (16, 8),\n",
    "            ],\n",
    "            \"mlpregressor__activation\": [\"relu\", \"tanh\"],\n",
    "            \"mlpregressor__alpha\": [0.0001, 0.001, 0.01],\n",
    "            \"mlpregressor__learning_rate_init\": [0.001, 0.01],\n",
    "            \"pca__n_components\": [0.98, 0.95, 0.9],\n",
    "        },\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc5c1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit every single circuit/GridSearch configuration\n",
    "models_and_circuits = {}\n",
    "\n",
    "for name in model_searches.keys():\n",
    "    models_and_circuits[name] = {}\n",
    "\n",
    "for circuit, data in dfs.items():\n",
    "    print(f\"Fitting models for {circuit}\")\n",
    "    circuit_start = time.time()\n",
    "\n",
    "    X, y = data.drop([\"LapTimeZScore\"], axis=\"columns\"), data[\"LapTimeZScore\"]\n",
    "    for name, model_search in model_searches.items():\n",
    "        print(f\"Fitting {name};\".ljust(50), end=\"\")\n",
    "        model_start = time.time()\n",
    "\n",
    "        model_search_copy = clone(model_search)\n",
    "        model_search_copy.fit(X, y)\n",
    "        models_and_circuits[name][circuit] = model_search_copy\n",
    "\n",
    "        print(f\"took {round(time.time() - model_start, 2)} seconds\")\n",
    "\n",
    "    print(\n",
    "        f'Took a total of {round(time.time() - circuit_start, 2)} seconds to fit all models for circuit \"{circuit}\"'\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098da9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models for later use\n",
    "context.catalog.save(\"initial_models\", models_and_circuits)\n",
    "print(\"Zapisano modele\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d81dcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show scores for each GridSearch and circuit\n",
    "all_scores = {}\n",
    "for key in models_and_circuits.keys():\n",
    "    scores = {}\n",
    "    for circuit, model in models_and_circuits[key].items():\n",
    "        scores[circuit] = model.best_score_\n",
    "    all_scores[key] = scores\n",
    "\n",
    "all_scores = pd.DataFrame(all_scores)\n",
    "\n",
    "all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa140e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show score statistics for each model\n",
    "# MinScore is very important. A good model should perform reasonably well for all tracks.\n",
    "model_scores_df = pd.DataFrame(\n",
    "    {\n",
    "        \"MeanScore\": all_scores.mean(axis=\"index\"),\n",
    "        \"MedianScore\": all_scores.median(axis=\"index\"),\n",
    "        \"ScoreVariance\": all_scores.var(axis=\"index\"),\n",
    "        \"MinScore\": all_scores.min(axis=\"index\"),\n",
    "    }\n",
    ")\n",
    "\n",
    "model_scores_df.sort_values(by=[\"MeanScore\"], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e302a1ed",
   "metadata": {},
   "source": [
    "## Result interpretation\n",
    "### The top-2\n",
    "**XGBRegressor is a clear winner**. The lowest score it got is over 0.64, mean and median scores are highest of all models, while score variance is low. It is clear that this algorithm reliably provides good results.\n",
    "\n",
    "**GradientBoostingRegressor** is a close runner up, with similar characteristics, albeit somewhat less accurate and less consistent. This does not come as a surprise, since it uses a similar but less advanced algorithm to XGBoost. \n",
    "\n",
    "### Remaining results\n",
    "The rest of the models have serious flaws. For example, **RandomForestRegressor**, despite having decent overall scores, has a higher score variance and got a score below 0.2 for one of the tracks. **ExtraTreesRegressor** is better in that regard, but still inferior to out top-2 models. \n",
    "\n",
    "The rest of the regressors perform significantly worse than the others, with versions of polynomial and linear regression having particularly low performance. There are some outlying values, even negative ones, in these models. Considering that XGBoost is a clear winner, I do not deem it necessary to look into this further at this point.\n",
    "\n",
    "### To sum up\n",
    "It appears that *boosting models*, particularly XGBRegressor and GradientBoostingRegressor, are the best. These are the models that will be optimized and tested further.\n",
    "\n",
    "<br><br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4189178f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how the best models perform on every circuit\n",
    "relevant_scores = all_scores.loc[:, [\"XGBRegressor\", \"GradientBoostingRegressor\"]]\n",
    "track_scores_df = pd.DataFrame(\n",
    "    {\n",
    "        \"MeanScore\": relevant_scores.mean(axis=\"columns\"),\n",
    "        \"XGBRegressorScore\": relevant_scores[\"XGBRegressor\"],\n",
    "        \"GradientBoostingRegressorScore\": relevant_scores[\"GradientBoostingRegressor\"],\n",
    "        \"DataPointCount\": [df.shape[0] for df in dfs.values()],\n",
    "    }\n",
    ")\n",
    "track_scores_df.sort_values(by=[\"MeanScore\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61fa454",
   "metadata": {},
   "source": [
    "## About scores by circuit\n",
    "Scores clearly vary a lot depending on the circuit. It is important to note that both the characteristics of the circuit itself, as well as how much data we have on each circuit has a big effect. For some circuits we only have data from one session, which is an obvious limitation and could affect score in different ways. The score tends to be higher for circuits with less than 2000 data points, possibly because the data points only come from one or two sessions in those cases. This makes it very likely for weather to be roughly constant throughout the data relevant to them, skewing CV results in favour of the model.\n",
    "\n",
    "### Point in favour of the results\n",
    "Even in the worst case, XGBoost had a mean score of over 0.64, which means it accounted for 64% of target attribute variance. <br> \n",
    "The target attribute in this case the driver's lap time z-score within each session. Z-score in this case basically denotes how good the lap was compared to the other laps the same driver completed in the same session. This means that for the most unpredictable circuit, our model accounted for 64% of how pit stops and weather affect driver performance. For over 70% of the circuits, the model accounted for over 80% of those differences.\n",
    "\n",
    "Therefore, it is clear to me that boosting models can produce decent-to-excellent results in general, even if some scores are exaggerated due to insufficient data size."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
